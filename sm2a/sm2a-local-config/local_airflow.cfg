[api]
auth_backends = airflow.api.auth.backend.basic_auth

[core]
executor = CeleryExecutor
dags_are_paused_at_creation = true
dags_folder = /opt/airflow/dags
load_examples = false
load_default_connections = false


[webserver]
dag_default_view = grid
expose_config = true
dag_orientation = TB
warn_deployment_exposure = false

[metrics]
# The airflow scheduler sends statsd metrics over UDP to port 8125.
# https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/logging-architecture.html
# You can verify this by setting the statsd_host to localhost or 0.0.0.0 and listening via netcat.
#   Eg. docker compose exec airflow-scheduler nc -l -u -p 8125 127.0.0.1
statsd_on = true
statsd_host = statsd-exporter
statsd_port = 8125
statsd_prefix = airflow

[scheduler]
catchup_by_default = false

[celery]

# This section only applies if you are using the CeleryExecutor in
# ``[core]`` section above
# The app name that will be used by celery
celery_app_name = airflow.executors.celery_executor

# The concurrency that will be used when starting workers with the
# ``airflow celery worker`` command. This defines the number of task instances that
# a worker will take, so size up your workers based on the resources on
# your worker box and the nature of your tasks
worker_concurrency = 16

# The maximum and minimum concurrency that will be used when starting workers with the
# ``airflow celery worker`` command (always keep minimum processes, but grow
# to maximum if necessary). Note the value should be max_concurrency,min_concurrency
# Pick these numbers based on resources on worker box and the nature of the task.
# If autoscale option is available, worker_concurrency will be ignored.
# http://docs.celeryproject.org/en/latest/reference/celery.bin.worker.html#cmdoption-celery-worker-autoscale
# Example: worker_autoscale = 16,12

# Used to increase the number of tasks that a worker prefetches which can improve performance.
# The number of processes multiplied by worker_prefetch_multiplier is the number of tasks
# that are prefetched by a worker. A value greater than 1 can result in tasks being unnecessarily
# blocked if there are multiple workers and one worker prefetches tasks that sit behind long
# running tasks while another worker has unutilized processes that are unable to process the already
# claimed blocked tasks.
# https://docs.celeryproject.org/en/stable/userguide/optimizing.html#prefetch-limits
# Example: worker_prefetch_multiplier = 1
# worker_prefetch_multiplier =

# Umask that will be used when starting workers with the ``airflow celery worker``
# in daemon mode. This control the file-creation mode mask which determines the initial
# value of file permission bits for newly created files.
worker_umask = 0o077
# celery_config_options = deploy_airflow_on_ecs_fargate.celery_config.CELERY_CONFIG

# The Celery broker URL. Celery supports RabbitMQ, Redis and experimentally
# a sqlalchemy database. Refer to the Celery documentation for more information.
# broker_url = redis://redis:6379/0
broker_url =  sqs://user:pass@celery-broker:9324/
#
# [logging]
# logging_config_class = deploy_airflow_on_ecs_fargate.logging_config.STDOUT_LOGGING_CONFIG

[secrets]
backend = airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend
backend_kwargs = {"connections_prefix": "sm2a-dev/airflow/connections", "variables_prefix": "sm2a-dev/airflow/variables","connections_lookup_pattern": "_default$", "variables_lookup_pattern": "^aws_", "config_prefix": "sm2a-dev/airflow/config"}
